{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4db4a86a",
   "metadata": {},
   "source": [
    "# Natural Language process - Gensim Python Library for text analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a71ecbb",
   "metadata": {},
   "source": [
    "# Introduction - Gensim is an acronym for Generate Similar. It is free python library for NLP. It is used in word embeddings, topic modeling and text similarity. It is developed for generating word and document vectors. It also extracts the topics from textual documents. It is an open-source, scalable, robust, fast, efficient multicore Implementation, and platform-independent. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2b91f390",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gensim\n",
      "  Downloading gensim-4.1.2-cp39-cp39-win_amd64.whl (24.0 MB)\n",
      "Requirement already satisfied: numpy>=1.17.0 in c:\\users\\ruchika gupta\\anaconda3\\lib\\site-packages (from gensim) (1.20.3)\n",
      "Collecting Cython==0.29.23\n",
      "  Downloading Cython-0.29.23-cp39-cp39-win_amd64.whl (1.7 MB)\n",
      "Collecting smart-open>=1.8.1\n",
      "  Downloading smart_open-5.2.1-py3-none-any.whl (58 kB)\n",
      "Requirement already satisfied: scipy>=0.18.1 in c:\\users\\ruchika gupta\\anaconda3\\lib\\site-packages (from gensim) (1.7.1)\n",
      "Installing collected packages: smart-open, Cython, gensim\n",
      "  Attempting uninstall: Cython\n",
      "    Found existing installation: Cython 0.29.24\n",
      "    Uninstalling Cython-0.29.24:\n",
      "      Successfully uninstalled Cython-0.29.24\n",
      "Successfully installed Cython-0.29.23 gensim-4.1.2 smart-open-5.2.1\n"
     ]
    }
   ],
   "source": [
    "# installing Gensim \n",
    "\n",
    "!pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "56e7714a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create gensim dictionary -\n",
    "#first we load the text data file.\n",
    "\n",
    "# open the text file \n",
    "file = open('hamlet.txt', encoding = 'utf-8')\n",
    "\n",
    "# read the file\n",
    "text = file.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b4a099a",
   "metadata": {},
   "source": [
    "# Now, we tokenize and preprocess the data using the string function split() and simple_preprocess() function available in gensim module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e1a606a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['the', 'tragedy', 'of', 'hamlet', 'prince', 'of', 'denmark', 'by', 'william', 'shakespeare', 'dramatis', 'personae', 'claudius', 'king', 'of', 'denmark'], ['marcellus', 'officer']]\n"
     ]
    }
   ],
   "source": [
    "# Tokenzie data : Handling punctuations and lowercasing the text\n",
    "from gensim.utils import simple_preprocess\n",
    "\n",
    "# preprocess the file to get a list of tokens\n",
    "token_list = []\n",
    "\n",
    "for sentence in text.split('.') :\n",
    "    token_list.append(simple_preprocess(sentence, deacc = True))\n",
    "    \n",
    "print(token_list[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d50634c",
   "metadata": {},
   "source": [
    "# After tokenization and preprocessing, we will create gensim dictionary object for the above-tokenized text.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a0f71337",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(4593 unique tokens: ['by', 'claudius', 'denmark', 'dramatis', 'hamlet']...)\n"
     ]
    }
   ],
   "source": [
    "# import gensim corpora\n",
    "from gensim import corpora\n",
    "\n",
    "#storing the extracted tokens into the dictionary\n",
    "my_dictionary = corpora.Dictionary(token_list)\n",
    "\n",
    "#print the  dictionary\n",
    "print(my_dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d1a1f83",
   "metadata": {},
   "source": [
    "# Now, we will see how to save and load the dictionary object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ffad6eb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(4593 unique tokens: ['by', 'claudius', 'denmark', 'dramatis', 'hamlet']...)\n"
     ]
    }
   ],
   "source": [
    "# save your dictionary to disk \n",
    "my_dictionary.save('dictionary.dict')\n",
    "\n",
    "#load back\n",
    "load_dict = corpora.Dictionary.load('dictionary.dict')\n",
    "print(load_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "128c925a",
   "metadata": {},
   "source": [
    "# Bag of Words\n",
    "The Bag-of-words model(BoW ) is the simplest way of extracting features from the text. BoW converts text into the matrix of the occurrence of words within a document. This model concerns whether given words occurred or not in the document."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "451ba5ac",
   "metadata": {},
   "source": [
    "# Letâ€™s create a bag of words using function doc2bow() for each tokenized sentence. Finally, we will have a list of tokens with their frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b789397b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[(0, 1), (1, 1), (2, 2), (3, 1), (4, 1), (5, 1), (6, 3), (7, 1), (8, 1), (9, 1), (10, 1), (11, 1), (12, 1)], [(13, 1), (14, 1)]]\n"
     ]
    }
   ],
   "source": [
    "BoW_corpus = [my_dictionary.doc2bow(sent, allow_update = True) for sent in token_list]\n",
    "print(BoW_corpus[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb5ec94d",
   "metadata": {},
   "source": [
    "# we can see the index and frequency of each token. If you want to replace the index with a token then you can try the following script:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "26a89342",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['by', 1], ['claudius', 1], ['denmark', 2], ['dramatis', 1], ['hamlet', 1], ['king', 1], ['of', 3], ['personae', 1], ['prince', 1], ['shakespeare', 1]]\n"
     ]
    }
   ],
   "source": [
    "# word weight in bag of words corpus\n",
    "\n",
    "word_weight = []\n",
    "\n",
    "for doc in BoW_corpus :\n",
    "    for id, freq in doc :\n",
    "        word_weight.append([my_dictionary[id], freq])\n",
    "        \n",
    "print(word_weight[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4561b4ae",
   "metadata": {},
   "source": [
    "# TF-IDF\n",
    "In Term Frequency(TF), you just count the number of words that occurred in each document. The main issue with this Term Frequency is that it will give more weight to longer documents. Term frequency is basically the output of the BoW model.\n",
    "IDF(Inverse Document Frequency) measures the amount of information a given word provides across the document. IDF is the logarithmically scaled inverse ratio of the number of documents that contain the word and the total number of documents.\n",
    "TF-IDF(Term Frequency-Inverse Document Frequency) normalizes the document term matrix. It is the product of TF and IDF. Word with high tf-idf in a document, it is most of the time that occurred in given documents and must be absent in the other documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9c49d97a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['by', 0.146], ['claudius', 0.31], ['denmark', 0.407], ['dramatis', 0.339], ['hamlet', 0.142], ['king', 0.117], ['of', 0.241], ['personae', 0.339], ['prince', 0.272], ['shakespeare', 0.339]]\n"
     ]
    }
   ],
   "source": [
    "import gensim.downloader as api\n",
    "from gensim import models\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "#create TF-IDF model\n",
    "tfIdf = models.TfidfModel(BoW_corpus, smartirs ='ntc')\n",
    "# TF - IDF word weight\n",
    "weight_tfidf = []\n",
    "for doc in tfIdf[BoW_corpus]:\n",
    "    for id, tf_idf in doc:\n",
    "        weight_tfidf.append([my_dictionary[id], np.around(tf_idf, decimals =3)])\n",
    "\n",
    "print(weight_tfidf[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ee6604a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
