{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9e4812ab",
   "metadata": {},
   "source": [
    "# Analyzing and Processing Text With SpaCy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c92159c0",
   "metadata": {},
   "source": [
    "# spaCy\n",
    "It is an open-source natural language processing library for Python. It is designed particularly for production use, and it can help us to build applications that process massive volumes of text efficiently"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "009fafcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting spacy\n",
      "  Downloading spacy-3.2.4-cp39-cp39-win_amd64.whl (11.3 MB)\n",
      "Requirement already satisfied: click<8.1.0 in c:\\users\\ruchika gupta\\anaconda3\\lib\\site-packages (from spacy) (8.0.3)\n",
      "Collecting spacy-legacy<3.1.0,>=3.0.8\n",
      "  Downloading spacy_legacy-3.0.9-py2.py3-none-any.whl (20 kB)\n",
      "Collecting murmurhash<1.1.0,>=0.28.0\n",
      "  Downloading murmurhash-1.0.6-cp39-cp39-win_amd64.whl (21 kB)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\users\\ruchika gupta\\anaconda3\\lib\\site-packages (from spacy) (1.20.3)\n",
      "Collecting thinc<8.1.0,>=8.0.12\n",
      "  Downloading thinc-8.0.15-cp39-cp39-win_amd64.whl (1.0 MB)\n",
      "Collecting pathy>=0.3.5\n",
      "  Downloading pathy-0.6.1-py3-none-any.whl (42 kB)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\ruchika gupta\\anaconda3\\lib\\site-packages (from spacy) (2.26.0)\n",
      "Collecting catalogue<2.1.0,>=2.0.6\n",
      "  Downloading catalogue-2.0.7-py3-none-any.whl (17 kB)\n",
      "Collecting cymem<2.1.0,>=2.0.2\n",
      "  Downloading cymem-2.0.6-cp39-cp39-win_amd64.whl (36 kB)\n",
      "Collecting typer<0.5.0,>=0.3.0\n",
      "  Downloading typer-0.4.1-py3-none-any.whl (27 kB)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\ruchika gupta\\anaconda3\\lib\\site-packages (from spacy) (2.11.3)\n",
      "Collecting spacy-loggers<2.0.0,>=1.0.0\n",
      "  Downloading spacy_loggers-1.0.2-py3-none-any.whl (7.2 kB)\n",
      "Collecting preshed<3.1.0,>=3.0.2\n",
      "  Downloading preshed-3.0.6-cp39-cp39-win_amd64.whl (112 kB)\n",
      "Collecting blis<0.8.0,>=0.4.0\n",
      "  Downloading blis-0.7.7-cp39-cp39-win_amd64.whl (6.6 MB)\n",
      "Collecting langcodes<4.0.0,>=3.2.0\n",
      "  Downloading langcodes-3.3.0-py3-none-any.whl (181 kB)\n",
      "Collecting srsly<3.0.0,>=2.4.1\n",
      "  Downloading srsly-2.4.2-cp39-cp39-win_amd64.whl (451 kB)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\ruchika gupta\\anaconda3\\lib\\site-packages (from spacy) (4.62.3)\n",
      "Collecting wasabi<1.1.0,>=0.8.1\n",
      "  Downloading wasabi-0.9.1-py3-none-any.whl (26 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\ruchika gupta\\anaconda3\\lib\\site-packages (from spacy) (21.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\ruchika gupta\\anaconda3\\lib\\site-packages (from spacy) (58.0.4)\n",
      "Collecting pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4\n",
      "  Downloading pydantic-1.8.2-cp39-cp39-win_amd64.whl (1.9 MB)\n",
      "Requirement already satisfied: colorama in c:\\users\\ruchika gupta\\anaconda3\\lib\\site-packages (from click<8.1.0->spacy) (0.4.4)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\users\\ruchika gupta\\anaconda3\\lib\\site-packages (from packaging>=20.0->spacy) (3.0.4)\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in c:\\users\\ruchika gupta\\anaconda3\\lib\\site-packages (from pathy>=0.3.5->spacy) (5.2.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\ruchika gupta\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4->spacy) (3.10.0.2)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\ruchika gupta\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\ruchika gupta\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.2)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\ruchika gupta\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\ruchika gupta\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2021.10.8)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\ruchika gupta\\anaconda3\\lib\\site-packages (from jinja2->spacy) (1.1.1)\n",
      "Installing collected packages: murmurhash, cymem, catalogue, wasabi, typer, srsly, pydantic, preshed, blis, thinc, spacy-loggers, spacy-legacy, pathy, langcodes, spacy\n",
      "Successfully installed blis-0.7.7 catalogue-2.0.7 cymem-2.0.6 langcodes-3.3.0 murmurhash-1.0.6 pathy-0.6.1 preshed-3.0.6 pydantic-1.8.2 spacy-3.2.4 spacy-legacy-3.0.9 spacy-loggers-1.0.2 srsly-2.4.2 thinc-8.0.15 typer-0.4.1 wasabi-0.9.1\n",
      "Collecting en-core-web-sm==3.2.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.2.0/en_core_web_sm-3.2.0-py3-none-any.whl (13.9 MB)\n",
      "Requirement already satisfied: spacy<3.3.0,>=3.2.0 in c:\\users\\ruchika gupta\\anaconda3\\lib\\site-packages (from en-core-web-sm==3.2.0) (3.2.4)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in c:\\users\\ruchika gupta\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.8.2)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\ruchika gupta\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.3.0)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\users\\ruchika gupta\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.20.3)\n",
      "Requirement already satisfied: pathy>=0.3.5 in c:\\users\\ruchika gupta\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (0.6.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\ruchika gupta\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.11.3)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.8 in c:\\users\\ruchika gupta\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.0.9)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in c:\\users\\ruchika gupta\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (0.7.7)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\ruchika gupta\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.26.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\ruchika gupta\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (21.0)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\ruchika gupta\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.0.6)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\ruchika gupta\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.0.2)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\ruchika gupta\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.0.7)\n",
      "Requirement already satisfied: click<8.1.0 in c:\\users\\ruchika gupta\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (8.0.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.1 in c:\\users\\ruchika gupta\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.4.2)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\ruchika gupta\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (4.62.3)\n",
      "Requirement already satisfied: thinc<8.1.0,>=8.0.12 in c:\\users\\ruchika gupta\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (8.0.15)\n",
      "Requirement already satisfied: typer<0.5.0,>=0.3.0 in c:\\users\\ruchika gupta\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (0.4.1)\n",
      "Requirement already satisfied: setuptools in c:\\users\\ruchika gupta\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (58.0.4)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\ruchika gupta\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.0.6)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\ruchika gupta\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.0.6)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in c:\\users\\ruchika gupta\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (0.9.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\ruchika gupta\\anaconda3\\lib\\site-packages (from click<8.1.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (0.4.4)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\users\\ruchika gupta\\anaconda3\\lib\\site-packages (from packaging>=20.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.0.4)\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in c:\\users\\ruchika gupta\\anaconda3\\lib\\site-packages (from pathy>=0.3.5->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (5.2.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\ruchika gupta\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.10.0.2)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\ruchika gupta\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\ruchika gupta\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.2)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\ruchika gupta\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.26.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\ruchika gupta\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2021.10.8)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\ruchika gupta\\anaconda3\\lib\\site-packages (from jinja2->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.1.1)\n",
      "Installing collected packages: en-core-web-sm\n",
      "Successfully installed en-core-web-sm-3.2.0\n",
      "[!] As of spaCy v3.0, shortcuts like 'en' are deprecated. Please use the full\n",
      "pipeline package name 'en_core_web_sm' instead.\n",
      "[+] Download and installation successful\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "!pip install spacy\n",
    "!python -m spacy download en"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9fa4b18",
   "metadata": {},
   "source": [
    "Tokenizing the Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bd1ebef8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['When', 'learning', 'data', 'science', ',', 'you', 'should', \"n't\", 'grt', 'discouraged', '!', 'challenges', 'and', 'setbacks', 'are', \"n't\", 'failures', ',', 'they', \"'re\", 'just', 'part', 'of', 'the', 'journey', '.']\n"
     ]
    }
   ],
   "source": [
    "# word tokenization \n",
    "\n",
    "from spacy.lang.en import English\n",
    "\n",
    "#Load English tokenizer, tagger, parser, NER and word vectors\n",
    "nlp = English()\n",
    "\n",
    "text = \"\"\"When learning data science, you shouldn't grt discouraged! challenges and setbacks aren't failures, they're just part of the journey.\"\"\"\n",
    "\n",
    "#\"nlp\" Object is used to create documnets with linguistic annotations.\n",
    "my_doc = nlp(text)\n",
    "\n",
    "# create list of word tokens\n",
    "token_list = []\n",
    "\n",
    "for token in my_doc:\n",
    "    token_list.append(token.text)\n",
    "    \n",
    "print(token_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15d5d8c3",
   "metadata": {},
   "source": [
    "Sentence Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5cd451b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"When learning data science, you shouldn't get discouraged!\", \"Challenges and setbacks aren't failures, they're just part of the journey.\", \"You've got this!\"]\n"
     ]
    }
   ],
   "source": [
    "# Load English tokenizer, tagger, parser, NER and word vectors\n",
    "nlp = English()\n",
    "\n",
    "# Add the 'sentencizer' component to the pipeline\n",
    "nlp.add_pipe('sentencizer')\n",
    "\n",
    "text = \"\"\"When learning data science, you shouldn't get discouraged! Challenges and setbacks aren't failures, they're just part of the journey. You've got this!\"\"\"\n",
    "\n",
    "#  \"nlp\" Object is used to create documents with linguistic annotations.\n",
    "doc = nlp(text)\n",
    "\n",
    "# create list of sentence tokens\n",
    "sents_list = []\n",
    "\n",
    "for sent in doc.sents:\n",
    "    sents_list.append(sent.text)\n",
    "    \n",
    "print(sents_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0a1031d",
   "metadata": {},
   "source": [
    "# Cleaning Text Data: Removing Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5d0c0aae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of stop words: 326\n",
      "First 20 stop words: ['have', 'whither', 'the', 'whoever', 'never', 'while', 'side', 'several', 'via', 'as', 'our', 'behind', 'almost', 'no', 'full', 'up', 'alone', 'mostly', 'upon', 'quite']\n"
     ]
    }
   ],
   "source": [
    "#Stop words\n",
    "\n",
    "#importing stop words from English language.\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "\n",
    "#Printing the total number of stop words:\n",
    "print('Number of stop words: %d' % len(STOP_WORDS))\n",
    "\n",
    "#Printing first twenty stop words:\n",
    "print('First 20 stop words: %s' % list(STOP_WORDS)[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1efe864",
   "metadata": {},
   "source": [
    "Removing Stopwords from Our Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "129d4ea7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered Sentence: [learning, data, science, discouraged, Challenges, setbacks, failures, journey, got]\n"
     ]
    }
   ],
   "source": [
    "from spacy.lang.en import English\n",
    "\n",
    "# Load English tokenizer, tagger, parser, NER and word vectors\n",
    "nlp = English()\n",
    "\n",
    "#  \"nlp\" Object is used to create documents with linguistic annotations.\n",
    "doc = nlp(text)\n",
    "\n",
    "filtered_tokens=[]\n",
    "\n",
    "# filtering stop words and punctuations\n",
    "for word in doc:\n",
    "    if word.is_stop==False:\n",
    "        if word.is_punct==False:\n",
    "            filtered_tokens.append(word)\n",
    "\n",
    "print(\"Filtered Sentence:\",filtered_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "620760b0",
   "metadata": {},
   "source": [
    "Lexicon Normalization -Lexicon normalization is another step in the text data cleaning process. In the big picture, normalization converts high dimensional features into low dimensional features that are appropriate for any machine learning model. For our purposes here, weâ€™re only going to look at lemmatization, a way of processing words that reduces them to their roots."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8c4830c",
   "metadata": {},
   "source": [
    "# Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c3a1f58a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run ==> run\n",
      "runs ==> run\n",
      "running ==> run\n",
      "runner ==> runner\n"
     ]
    }
   ],
   "source": [
    "# importing the model en_core_web_sm of English for vocabluary, syntax & entities\n",
    "import en_core_web_sm\n",
    "\n",
    "# load en_core_web_sm of English for vocabluary, syntax & entities\n",
    "nlp = en_core_web_sm.load()\n",
    "\n",
    "# Implementing lemmatization\n",
    "lem = nlp(\"run runs running runner\")\n",
    "\n",
    "# finding lemma for each word\n",
    "for word in lem:\n",
    "    print(word.text,\"==>\" ,word.lemma_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "60015764",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered Tokens: [learning, data, science, discouraged, Challenges, setbacks, failures, journey, got]\n",
      "Lemmatized Tokens: ['learn', 'datum', 'science', 'discourage', 'challenge', 'setback', 'failure', 'journey', 'get']\n"
     ]
    }
   ],
   "source": [
    "# importing the model en_core_web_sm of English for vocabluary, syntax & entities\n",
    "import en_core_web_sm\n",
    "\n",
    "# load en_core_web_sm of English for vocabluary, syntax & entities\n",
    "nlp = en_core_web_sm.load()\n",
    "\n",
    "text = \"\"\"When learning data science, you shouldn't get discouraged! Challenges and setbacks aren't failures, they're just part of the journey. You've got this!\"\"\"\n",
    "\n",
    "#  \"nlp\" Object is used to create documents with linguistic annotations.\n",
    "doc = nlp(text)\n",
    "\n",
    "filtered_tokens=[]\n",
    "\n",
    "# filtering stop words and punctuations\n",
    "for word in doc:\n",
    "    if word.is_stop==False:\n",
    "        if word.is_punct==False:\n",
    "            filtered_tokens.append(word)\n",
    "\n",
    "print(\"Filtered Tokens:\",filtered_tokens)\n",
    "\n",
    "normalized_tokens=[]\n",
    "for token in filtered_tokens:\n",
    "    normalized_tokens.append(token.lemma_)\n",
    "    \n",
    "print(\"Lemmatized Tokens:\",normalized_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba6bb5c3",
   "metadata": {},
   "source": [
    "# Part of Speech (POS) Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d922ac16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All PRON\n",
      "is AUX\n",
      "well ADJ\n",
      "that PRON\n",
      "ends VERB\n",
      "well ADV\n",
      ". PUNCT\n"
     ]
    }
   ],
   "source": [
    "# POS tagging\n",
    "\n",
    "# importing the model en_core_web_sm of English for vocabluary, syntax & entities\n",
    "import en_core_web_sm   \n",
    "\n",
    "# load en_core_web_sm of English for vocabluary, syntax & entities\n",
    "nlp = en_core_web_sm.load()  \n",
    "\n",
    "# \"nlp\" Objectis used to create documents with linguistic annotations.\n",
    "docs = nlp(u\"All is well that ends well.\")\n",
    "\n",
    "for word in docs:\n",
    "    print(word.text,word.pos_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6f56934",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
